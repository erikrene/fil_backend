{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ad97cc",
   "metadata": {},
   "source": [
    "# Ensembling with Triton-FIL\n",
    "\n",
    "## Introduction\n",
    "This notebook will go through step-by-step the process of training an ensemble of models and deploying it to Triton's new FIL backend. Additionally, we will investigate how to handle an ensemble of models with Triton using the python backend.\n",
    "## Pre-Requisites\n",
    "This notebook assumes that you have Docker plus a few Python dependencies. To install all of these dependencies in a conda environment, you may make use of the following conda environment file:\n",
    "```yaml\n",
    "---\n",
    "name: triton_ensemble_nb\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - nvidia\n",
    "  - rapidsai\n",
    "dependencies:\n",
    "  - conda-pack\n",
    "  - cudatoolkit=11.4\n",
    "  - cudf=21.12\n",
    "  - cuml=21.12\n",
    "  - cupy\n",
    "  - jupyter\n",
    "  - kaggle\n",
    "  - matplotlib\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - pip\n",
    "  - python=3.8\n",
    "  - scikit-learn\n",
    "  - pip:\n",
    "      - treelite=2.3.0\n",
    "      - tritonclient[all]\n",
    "      - xgboost>=1.5,<1.6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRITON_IMAGE = 'nvcr.io/nvidia/tritonserver:22.05-py3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190453b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull {TRITON_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ae4a2",
   "metadata": {},
   "source": [
    "## Fetching Training Data\n",
    "For this example, we will make use of data from the [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection/overview) Kaggle competition. You may fetch the data from this competition using the Kaggle command line client using the following commands.\n",
    "\n",
    "\n",
    "**NOTE**: You will need to make sure that your Kaggle credentials are [available](https://github.com/Kaggle/kaggle-api#api-credentials) either through a kaggle.json file or via environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd826c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kaggle competitions download -c ieee-fraud-detection\n",
    "!unzip -u ieee-fraud-detection.zip\n",
    "train_csv = 'train_transaction.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0fb66a",
   "metadata": {},
   "source": [
    "## Training Example Models\n",
    "While the IEEE-CIS Kaggle competition focused on a more sophisticated problem involving analysis of both fraudulent transactions and the users linked to those transactions, we will use a simpler version of that problem (identifying fraudulent transactions only) to build our example model. In the following steps, we make use of cuML's preprocessing tools to clean the data and then train two example models using XGBoost. Note that we will be making use of the new categorical feature support in XGBoost 1.5. If you wish to use an earlier version of XGBoost, you will need to perform a [label encoding](https://docs.rapids.ai/api/cuml/stable/api.html?highlight=labelencoder#cuml.preprocessing.LabelEncoder.LabelEncoder) on the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.preprocessing import SimpleImputer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b440e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading data\n",
    "data = cudf.read_csv(train_csv)\n",
    "\n",
    "# Replace NaNs in data\n",
    "nan_columns = data.columns[data.isna().any().to_pandas()]\n",
    "float_nan_subset = data[nan_columns].select_dtypes(include='float64')\n",
    "\n",
    "imputer = SimpleImputer(missing_values=cp.nan, strategy='mean')\n",
    "data[float_nan_subset.columns] = imputer.fit_transform(float_nan_subset)\n",
    "\n",
    "obj_nan_subset = data[nan_columns].select_dtypes(include='object')\n",
    "data[obj_nan_subset.columns] = obj_nan_subset.fillna('UNKNOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform label encoding\n",
    "cat_columns = data.select_dtypes(include='object')\n",
    "for col in cat_columns.columns:\n",
    "    data[col] = LabelEncoder().fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f51e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = data.drop('isFraud', axis=1)\n",
    "y = data.isFraud.astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.to_pandas(), y.to_pandas(), test_size=0.3, stratify=y.to_pandas(), random_state=SEED\n",
    ")\n",
    "# Copy data to avoid slowdowns due to fragmentation\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model training functions\n",
    "def train_model_logistic(num_trees, max_depth):\n",
    "    model = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',\n",
    "        enable_categorical=False,\n",
    "        use_label_encoder=False,\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        objective='binary:logistic',\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=num_trees\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_oversample(num_trees, max_depth):\n",
    "    model = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',\n",
    "        enable_categorical=False,\n",
    "        use_label_encoder=False,\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        objective='binary:logistic',\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=num_trees\n",
    "    )\n",
    "    \n",
    "    oversample = RandomOverSampler(sampling_strategy=0.5) # Define oversampling strategy\n",
    "    X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "    \n",
    "    model.fit(\n",
    "        X_over,\n",
    "        y_over,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_RFC(num_trees, max_depth):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=num_trees,\n",
    "        max_depth=max_depth,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29691d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = train_model_logistic(1500, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10275e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_oversample = train_model_oversample(500, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6956b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RFC = train_model_RFC(40, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some room on the GPU by explicitly deleting dataframes\n",
    "import gc\n",
    "del data\n",
    "del nan_columns\n",
    "del float_nan_subset\n",
    "del imputer\n",
    "del obj_nan_subset\n",
    "del cat_columns\n",
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3008720",
   "metadata": {},
   "source": [
    "## Deploying Models in Triton\n",
    "Now that we have two example models to work with, let's actually deploy them for real-time serving using Triton. In order to do so, we will need to first serialize the models in the directory structure that Triton expects and then add configuration files to tell Triton exactly how we wish to use these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e0a5c",
   "metadata": {},
   "source": [
    "### Model Serialization\n",
    "Triton models can be stored locally on disk or in S3, Google Cloud Storage, or Azure Storage. For this example, we will stick to local storage, but information about using cloud storage solutions can be found [here](https://github.com/triton-inference-server/server/blob/main/docs/model_repository.md). Each model has a dedicated directory within a main model repository directory. Multiple versions of a model can also be served by Triton, as indicated by numbered directories (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a005e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import treelite\n",
    "import pickle\n",
    "\n",
    "# Create the model repository directory. The name of this directory is arbitrary.\n",
    "REPO_PATH = os.path.abspath('model_repository')\n",
    "os.makedirs(REPO_PATH, exist_ok=True)\n",
    "\n",
    "# We will use the following variables to record information from the serialization\n",
    "# process that we will require later\n",
    "model_path = None\n",
    "model_format = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee0f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_model_xgb(model, model_name):\n",
    "    # The name of the model directory determines the name of the model as reported\n",
    "    # by Triton\n",
    "    model_dir = os.path.join(REPO_PATH, model_name)\n",
    "    # We can store multiple versions of the model in the same directory. In our\n",
    "    # case, we have just one version, so we will add a single directory, named '1'.\n",
    "    version_dir = os.path.join(model_dir, '1')\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    # The default filename for XGBoost models saved in json format is 'xgboost.json'.\n",
    "    # It is recommended that you use this filename to avoid having to specify a\n",
    "    # name in the configuration file.\n",
    "    model_file = os.path.join(version_dir, 'xgboost.json')\n",
    "    model.save_model(model_file)\n",
    "    \n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_model_skl(model, model_name):\n",
    "    model_dir = os.path.join(REPO_PATH, model_name)\n",
    "    version_dir = os.path.join(model_dir, '1')\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    archival_path = os.path.join(version_dir, 'model.pkl')\n",
    "    with open(archival_path,\"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    # This is the default filename expected for Treelite checkpoint models. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_basename = 'checkpoint.tl'\n",
    "    model_file = os.path.join(version_dir, model_basename)\n",
    "        \n",
    "    tl_model = treelite.sklearn.import_model(model)\n",
    "    tl_model.serialize(model_file)\n",
    "    \n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d42c83",
   "metadata": {},
   "source": [
    "We will be deploying two copies of each of our example models: one on CPU and one on GPU. We will use these separate instances to demonstrate the performance differences between GPU and CPU execution later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33ec4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_logistic_dir = serialize_model_xgb(model_logistic, 'model_logistic')\n",
    "model_logistic_cpu_dir = serialize_model_xgb(model_logistic, 'model_logistic-cpu')\n",
    "model_oversample_dir = serialize_model_xgb(model_oversample, 'model_oversample')\n",
    "model_oversample_cpu_dir = serialize_model_xgb(model_oversample, 'model_oversample-cpu')\n",
    "model_RFC_dir = serialize_model_skl(model_RFC, 'model_RFC')\n",
    "model_RFC_cpu_dir = serialize_model_skl(model_RFC, 'model_RFC-cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550c7a9",
   "metadata": {},
   "source": [
    "### The Configuration File\n",
    "The configuration file associated with a model tells Triton a little bit about the model itself and how you would like to use it. You can read about all generic Triton configuration options [here](https://github.com/triton-inference-server/server/blob/master/docs/model_configuration.md) and about configuration options specific to the FIL backend [here](https://github.com/triton-inference-server/fil_backend#configuration), but we will focus on just a few of the most common and relevant options in this example. Below are general descriptions of these options:\n",
    "- **max_batch_size**: The maximum batch size that can be passed to this model. In general, the only limit on the size of batches passed to a FIL backend is the memory available with which to process them. For GPU execution, the available memory is determined by the size of Triton's CUDA memory pool, which can be set via a command line argument when starting the server.\n",
    "- **input**: Options in this section tell Triton the number of features to expect for each input sample.\n",
    "- **output**: Options in this section tell Triton how many output values there will be for each sample. If the \"predict_proba\" option (described further on) is set to true, then a probability value will be returned for each class. Otherwise, a single value will be returned indicating the class predicted for the given sample.\n",
    "- **instance_group**: This determines how many instances of this model will be created and whether they will use the GPU or CPU.\n",
    "- **model_type**: A string indicating what format the model is in (\"xgboost_json\" in this example, but \"xgboost\", \"lightgbm\", and \"tl_checkpoint\" are valid formats as well).\n",
    "- **predict_proba**: If set to true, probability values will be returned for each class rather than just a class prediction.\n",
    "- **output_class**: True for classification models, false for regression models.\n",
    "- **threshold**: A score threshold for determining classification. When output_class is set to true, this must be provided, although it will not be used if predict_proba is also set to true.\n",
    "- **storage_type**: In general, using \"AUTO\" for this setting should meet most usecases. If \"AUTO\" storage is selected, FIL will load the model using either a sparse or dense representation based on the approximate size of the model. In some cases, you may want to explicitly set this to \"SPARSE\" in order to reduce the memory footprint of large models.\n",
    "\n",
    "Based on this information, let's set up configuration files for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858fd518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum size in bytes for input and output arrays. If you are\n",
    "# using Triton 21.11 or higher, all memory allocations will make\n",
    "# use of Triton's memory pool, which has a default size of\n",
    "# 67_108_864 bytes. This can be increased using the\n",
    "# `--cuda-memory-pool-byte-size` option when the server is\n",
    "# started, but this notebook should work fine with default\n",
    "# settings.\n",
    "MAX_MEMORY_BYTES = 60_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ab7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_test.shape[1]\n",
    "num_classes = cp.unique(y_test).size\n",
    "bytes_per_sample = (features + num_classes) * 4\n",
    "max_batch_size = MAX_MEMORY_BYTES // bytes_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e794668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_config(model_dir, model_format, storage_type, deployment_type='gpu'):\n",
    "    if deployment_type.lower() == 'cpu':\n",
    "        instance_kind = 'KIND_CPU'\n",
    "    else:\n",
    "        instance_kind = 'KIND_GPU'\n",
    "        \n",
    "    config_text = f\"\"\"backend: \"fil\"\n",
    "max_batch_size: {max_batch_size}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {features} ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {num_classes} ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: {instance_kind} }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"{model_format}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"{storage_type}\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\"\"\"\n",
    "    config_path = os.path.join(model_dir, 'config.pbtxt')\n",
    "    with open(config_path, 'w') as file_:\n",
    "        file_.write(config_text)\n",
    "\n",
    "    return config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43016177",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_config(model_logistic_dir, deployment_type='gpu', model_format='xgboost_json', storage_type='AUTO')\n",
    "generate_config(model_logistic_cpu_dir, deployment_type='cpu', model_format='xgboost_json', storage_type='AUTO')\n",
    "generate_config(model_oversample_dir, deployment_type='gpu', model_format='xgboost_json', storage_type='SPARSE')\n",
    "generate_config(model_oversample_cpu_dir, deployment_type='cpu', model_format='xgboost_json', storage_type='SPARSE')\n",
    "generate_config(model_RFC_dir, deployment_type='gpu', model_format='treelite_checkpoint', storage_type='AUTO')\n",
    "generate_config(model_RFC_cpu_dir, deployment_type='cpu', model_format='treelite_checkpoint', storage_type='AUTO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd8a43",
   "metadata": {},
   "source": [
    "### Python Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264181f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_text = f\"\"\"import triton_python_backend_utils as pb_utils\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "class TritonPythonModel:\n",
    "    def initialize(self, args):\n",
    "\n",
    "\n",
    "        # You must parse model_config. JSON string is not parsed here\n",
    "        self.model_config = json.loads(args['model_config'])\n",
    "\n",
    "        # Get OUTPUT0 configuration\n",
    "        self.output0_config = pb_utils.get_output_config_by_name(\n",
    "            self.model_config, \"output__0\")\n",
    "        \n",
    "        self.output0_dtype = pb_utils.triton_string_to_numpy(\n",
    "            self.output0_config['data_type'])\n",
    "\n",
    "    # You must add the Python 'async' keyword to the beginning of `execute`\n",
    "    # function if you want to use `async_exec` function.\n",
    "    async def execute(self, requests):\n",
    "        output0_dtype = self.output0_dtype\n",
    "\n",
    "        responses = []\n",
    "        # Every Python backend must iterate over everyone of the requests\n",
    "        # and create a pb_utils.InferenceResponse for each of them.\n",
    "        for request in requests:\n",
    "            # Get INPUT0\n",
    "            in_0 = pb_utils.get_input_tensor_by_name(request, \"input__0\")\n",
    "\n",
    "            # List of awaitables containing inflight inference responses.\n",
    "            inference_response_awaits = []\n",
    "            for model_name in ['model_logistic', 'model_oversample', 'model_RFC']:\n",
    "                # Create inference request object\n",
    "                infer_request = pb_utils.InferenceRequest(\n",
    "                    model_name=model_name,\n",
    "                    requested_output_names=[\"output__0\"],\n",
    "                    inputs=[in_0])\n",
    "\n",
    "                # Store the awaitable inside the array. We don't need\n",
    "                # the inference response immediately so we do not `await`\n",
    "                # here.\n",
    "                inference_response_awaits.append(infer_request.async_exec())\n",
    "\n",
    "            # Wait for all the inference requests to finish. The execution\n",
    "            # of the Python script will be blocked until all the awaitables\n",
    "            # are resolved.\n",
    "            inference_responses = await asyncio.gather(\n",
    "                *inference_response_awaits)\n",
    "\n",
    "            for infer_response in inference_responses:\n",
    "                # Make sure that the inference response doesn't have an error.\n",
    "                # If it has an error and you can't proceed with your model\n",
    "                # execution you can raise an exception.\n",
    "                if infer_response.has_error():\n",
    "                    raise pb_utils.TritonModelException(\n",
    "                        infer_response.error().message())\n",
    "\n",
    "            logistic_tensor = pb_utils.get_output_tensor_by_name(\n",
    "                inference_responses[0], \"output__0\")\n",
    "            logistic_tensor = from_dlpack(logistic_tensor.to_dlpack())\n",
    "\n",
    "            oversample_tensor = pb_utils.get_output_tensor_by_name(\n",
    "                inference_responses[1], \"output__0\")\n",
    "            oversample_tensor = from_dlpack(oversample_tensor.to_dlpack())\n",
    "            \n",
    "            RFC_tensor = pb_utils.get_output_tensor_by_name(\n",
    "                inference_responses[2], \"output__0\")\n",
    "            RFC_tensor = from_dlpack(RFC_tensor.to_dlpack())\n",
    "            \n",
    "            ensembled = (logistic_tensor.as_numpy() + oversample_tensor.as_numpy() + RFC_tensor.as_numpy()) / 3\n",
    "            ensembled_tensor = pb_utils.Tensor(\"output__0\", ensembled.astype(output0_dtype))\n",
    "\n",
    "            inference_response = pb_utils.InferenceResponse(\n",
    "                output_tensors=[ensembled_tensor])\n",
    "            responses.append(inference_response)\n",
    "\n",
    "        # You should return a list of pb_utils.InferenceResponse. Length\n",
    "        # of this list must match the length of `requests` list.\n",
    "        return responses\n",
    "\n",
    "    def finalize(self):\n",
    "        print('Cleaning up...')\"\"\"\n",
    "\n",
    "python_dir = os.path.join(REPO_PATH, 'model_ensemble')\n",
    "python_version_dir = os.path.join(python_dir, '1')\n",
    "os.makedirs(python_version_dir, exist_ok=True)\n",
    "\n",
    "python_path = os.path.join(python_version_dir, 'model.py')\n",
    "with open(python_path, 'w') as file_:\n",
    "    file_.write(python_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016e3d5",
   "metadata": {},
   "source": [
    "### Starting the server\n",
    "With valid models and configuration files in place, we can now start the server. Below, we do so, use the Python client to wait for it to come fully online, and then check the logs to make sure we didn't get any unexpected warnings or errors while loading the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e2f6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!docker run --gpus all -d -p 8000:8000 -p 8001:8001 -p 8002:8002 -v {REPO_PATH}:/models --name tritonserver {TRITON_IMAGE} tritonserver --model-repository=/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as triton_grpc\n",
    "from tritonclient import utils as triton_utils\n",
    "HOST = 'localhost'\n",
    "PORT = 8001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c7972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = triton_grpc.InferenceServerClient(url=f'{HOST}:{PORT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66742762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(5) # Wait for server to come online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e9196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker logs tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ae4ef",
   "metadata": {},
   "source": [
    "## Submitting inference requests\n",
    "With our models now deployed on a running Triton server, let's confirm that we get the same results from the deployed model as we get locally. Note that we will occasionally see slight divergences due to floating point errors during parallel execution, but otherwise, results should match.\n",
    "\n",
    "### Categorical variables\n",
    "If you are using a model with categorical features, a certain amount of care must be taken with categorical features, just as if you were executing a model locally. Both XGBoost and LightGBM depend on the input data frames to convert categories into numeric variables. If data is later submitted from a data frame which contains a different subset of categories, this numeric conversion will not be handled properly. In this example, we will use the same dataframe we used during testing, so we need not consider this, but otherwise we would need to note the mapping used for the `.codes` attribute for each categorical feature in the training dataframe and make sure the same codes were used when submitting inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64db65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def convert_to_numpy(df):\n",
    "    df = df.copy()\n",
    "    cat_cols = df.select_dtypes('category').columns\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].cat.codes\n",
    "    for col in df.columns:\n",
    "        df[col] =  pd.to_numeric(df[col], downcast='float')\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60633965",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = convert_to_numpy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261d84f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_name = \"model_ensemble\"\n",
    "\n",
    "input0_data = np_data[0:5]\n",
    "\n",
    "inputs = [\n",
    "    triton_grpc.InferInput(\"input__0\", input0_data.shape, 'FP32')\n",
    "]\n",
    "\n",
    "inputs[0].set_data_from_numpy(input0_data)\n",
    "\n",
    "outputs = [\n",
    "    triton_grpc.InferRequestedOutput(\"output__0\"),\n",
    "]\n",
    "\n",
    "response = client.infer(model_name,\n",
    "                        inputs=inputs,\n",
    "                        request_id=str(1),\n",
    "                        outputs=outputs)\n",
    "\n",
    "result = response.get_response()\n",
    "triton_result = response.as_numpy(\"output__0\")\n",
    "print(\"Result computed on Triton: \")\n",
    "print(triton_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d5046",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resulted computed locally: \")\n",
    "local_result = (model_logistic.predict_proba(X_test[0:5]) + model_oversample.predict_proba(X_test[0:5]) + model_RFC.predict_proba(X_test[0:5])) / 3\n",
    "print(local_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the server\n",
    "!docker rm -f tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698bc9c",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
