{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ad97cc",
   "metadata": {},
   "source": [
    "# Ensembling with Triton-FIL\n",
    "\n",
    "## Introduction\n",
    "This notebook will go through step-by-step the process of training an ensemble of models and deploying it to Triton's new FIL backend, building off of the [Fraud Detection Notebook](https://github.com/triton-inference-server/fil_backend/blob/main/notebooks/categorical-fraud-detection/Fraud_Detection_Example.ipynb). We will be utilizing Triton's Python Backend in order to combine the models. This requires the use of a Python model that executes inference requests on each model in the ensemble. This notebook will focus on the process of creating the Python model, as well as how to submit requests with the Triton Python client.\n",
    "\n",
    "__NOTE__: Currently, GPU support is not available for the Python backend. This is due to a DLPack update that has yet to be implemented. However, the process would be virtually the same, except the Python backend Tensor objects would have to be converted to DLPack with the methods found [here](https://github.com/triton-inference-server/fil_backend/blob/main/notebooks/categorical-fraud-detection/Fraud_Detection_Example.ipynb).\n",
    "\n",
    "## Pre-Requisites\n",
    "This notebook assumes that you have Docker plus a few Python dependencies, as in the Fraud Detection Notebook. However, this notebook utilizes Treelite, as we will be using Scikit-Learn models (these are not natively supported by the FIL backend, unlike XGBoost models). To install all of these dependencies in a conda environment, you may make use of the following conda environment file:\n",
    "```yaml\n",
    "---\n",
    "name: triton_ensemble_nb\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - nvidia\n",
    "  - rapidsai\n",
    "dependencies:\n",
    "  - cudatoolkit=11.4\n",
    "  - cudf=21.12\n",
    "  - cuml=21.12\n",
    "  - cupy\n",
    "  - jupyter\n",
    "  - kaggle\n",
    "  - matplotlib\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - pip\n",
    "  - python=3.8\n",
    "  - scikit-learn\n",
    "  - pip:\n",
    "      - treelite=2.3.0\n",
    "      - tritonclient[all]\n",
    "      - xgboost>=1.5,<1.6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29dd53a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TRITON_IMAGE = 'nvcr.io/nvidia/tritonserver:22.05-py3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190453b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.05-py3: Pulling from nvidia/tritonserver\r\n",
      "Digest: sha256:a85daa2907f46e70b3782818a0331df62d9b4e0b1f15f1530b2a52c8c782d46d\r\n",
      "Status: Image is up to date for nvcr.io/nvidia/tritonserver:22.05-py3\r\n",
      "nvcr.io/nvidia/tritonserver:22.05-py3\r\n"
     ]
    }
   ],
   "source": [
    "!docker pull {TRITON_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ae4a2",
   "metadata": {},
   "source": [
    "## Fetching Training Data\n",
    "As in the Fraud Detection Notebook, we will make use of data from the [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection/overview) Kaggle competition.\n",
    "\n",
    "**NOTE**: You will need to make sure that your Kaggle credentials are [available](https://github.com/Kaggle/kaggle-api#api-credentials) either through a kaggle.json file or via environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd826c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ieee-fraud-detection.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Archive:  ieee-fraud-detection.zip\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c ieee-fraud-detection\n",
    "!unzip -u ieee-fraud-detection.zip\n",
    "train_csv = 'train_transaction.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0fb66a",
   "metadata": {},
   "source": [
    "## Training Example Models\n",
    "We will be training two XGBoost models and one Scikit-Learn model for our ensemble. The first XGBoost model function is the same as the one from the fraud detection notebook. The second XGBoost function is similar to the first, except it implements random oversampling on the data. The third model is a Scikit-Learn Random Forest Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4557f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.preprocessing import SimpleImputer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b440e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading data\n",
    "data = cudf.read_csv(train_csv)\n",
    "\n",
    "# Replace NaNs in data\n",
    "nan_columns = data.columns[data.isna().any().to_pandas()]\n",
    "float_nan_subset = data[nan_columns].select_dtypes(include='float64')\n",
    "\n",
    "imputer = SimpleImputer(missing_values=cp.nan, strategy='mean')\n",
    "data[float_nan_subset.columns] = imputer.fit_transform(float_nan_subset)\n",
    "\n",
    "obj_nan_subset = data[nan_columns].select_dtypes(include='object')\n",
    "data[obj_nan_subset.columns] = obj_nan_subset.fillna('UNKNOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce2d23e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform label encoding\n",
    "cat_columns = data.select_dtypes(include='object')\n",
    "for col in cat_columns.columns:\n",
    "    data[col] = LabelEncoder().fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f51e89d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = data.drop('isFraud', axis=1)\n",
    "y = data.isFraud.astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.to_pandas(), y.to_pandas(), test_size=0.3, stratify=y.to_pandas(), random_state=SEED\n",
    ")\n",
    "# Copy data to avoid slowdowns due to fragmentation\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eebcb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aec11f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model_logistic(num_trees, max_depth):\n",
    "    model = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',\n",
    "        enable_categorical=False,\n",
    "        use_label_encoder=False,\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        objective='binary:logistic',\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=num_trees\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b60edad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model_oversample(num_trees, max_depth):\n",
    "    model = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',\n",
    "        enable_categorical=False,\n",
    "        use_label_encoder=False,\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        objective='binary:logistic',\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=num_trees\n",
    "    )\n",
    "    \n",
    "    oversample = RandomOverSampler(sampling_strategy=0.5) # Define oversampling strategy\n",
    "    X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "    \n",
    "    model.fit(\n",
    "        X_over,\n",
    "        y_over,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95b6328b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model_RFC(num_trees, max_depth):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=num_trees,\n",
    "        max_depth=max_depth,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29691d00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_logistic = train_model_logistic(1500, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10275e39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_oversample = train_model_oversample(500, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6956b8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_RFC = train_model_RFC(40, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65e8ab0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeing up room on GPU\n",
    "import gc\n",
    "del data\n",
    "del nan_columns\n",
    "del float_nan_subset\n",
    "del imputer\n",
    "del obj_nan_subset\n",
    "del cat_columns\n",
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3008720",
   "metadata": {},
   "source": [
    "## Preparing Models for Deployment\n",
    "The same process in the previous notebook will be used to prepare models to be deployed to Triton. First, we will serialize the models, then add the configuration files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e0a5c",
   "metadata": {},
   "source": [
    "### Model Serialization\n",
    "Once again, certain model formats are not natively supported by the FIL backend, so they must be directly serialized to Treelite's checkpoint format. Additionally, it is important to ensure that the correct filename is given to each model, depending on its format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a005e16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import treelite\n",
    "import pickle\n",
    "\n",
    "# Create the model repository directory. The name of this directory is arbitrary.\n",
    "REPO_PATH = os.path.abspath('model_repository')\n",
    "os.makedirs(REPO_PATH, exist_ok=True)\n",
    "\n",
    "# We will use the following variables to record information from the serialization\n",
    "# process that we will require later\n",
    "model_path = None\n",
    "model_format = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ee0f16d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def serialize_model_xgb(model, model_name):\n",
    "    model_dir = os.path.join(REPO_PATH, model_name) # Creating model repository\n",
    "    version_dir = os.path.join(model_dir, '1') # Creating version 1 directory\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    # This is the default filename for XGBoost models saved in json format. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_file = os.path.join(version_dir, 'xgboost.json')\n",
    "    model.save_model(model_file)\n",
    "    \n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db3e9d45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def serialize_model_skl(model, model_name):\n",
    "    model_dir = os.path.join(REPO_PATH, model_name)\n",
    "    version_dir = os.path.join(model_dir, '1')\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    # Since Treelite provides no compatibility guarantees between different versions, it is recommended that you\n",
    "    # save models in Pickle or Joblib.\n",
    "    archival_path = os.path.join(version_dir, 'model.pkl')\n",
    "    with open(archival_path,\"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    # This is the default filename expected for Treelite checkpoint models. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_file = os.path.join(version_dir, 'checkpoint.tl')\n",
    "        \n",
    "    tl_model = treelite.sklearn.import_model(model)\n",
    "    tl_model.serialize(model_file)\n",
    "    \n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf33ec4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/enarvades/miniconda3/envs/triton_ensemble_nb/lib/python3.8/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model_logistic_dir = serialize_model_xgb(model_logistic, 'model_logistic')\n",
    "model_logistic_cpu_dir = serialize_model_xgb(model_logistic, 'model_logistic-cpu')\n",
    "model_oversample_dir = serialize_model_xgb(model_oversample, 'model_oversample')\n",
    "model_oversample_cpu_dir = serialize_model_xgb(model_oversample, 'model_oversample-cpu')\n",
    "model_RFC_dir = serialize_model_skl(model_RFC, 'model_RFC')\n",
    "model_RFC_cpu_dir = serialize_model_skl(model_RFC, 'model_RFC-cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550c7a9",
   "metadata": {},
   "source": [
    "### The Configuration File\n",
    "We will set up the configuration file in the same manner as the previous notebook, except we will add a parameter for model format, since we're using multiple formats (in this case, XGBoost and Scikit-Learn). \n",
    "\n",
    "Once again, you can read about the FIL backend's configuration options [here](https://github.com/triton-inference-server/fil_backend#configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "539ab7b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = X_test.shape[1]\n",
    "num_classes = cp.unique(y_test).size\n",
    "bytes_per_sample = (features + num_classes) * 4\n",
    "max_batch_size = 60_000_000 // bytes_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e794668",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_config(model_dir, model_format, storage_type, deployment_type='gpu'):\n",
    "    if deployment_type.lower() == 'cpu':\n",
    "        instance_kind = 'KIND_CPU'\n",
    "    else:\n",
    "        instance_kind = 'KIND_GPU'\n",
    "        \n",
    "    config_text = f\"\"\"backend: \"fil\"\n",
    "max_batch_size: {max_batch_size}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {features} ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {num_classes} ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: {instance_kind} }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"{model_format}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"{storage_type}\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\"\"\"\n",
    "    config_path = os.path.join(model_dir, 'config.pbtxt')\n",
    "    with open(config_path, 'w') as file_:\n",
    "        file_.write(config_text)\n",
    "\n",
    "    return config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43016177",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nfs/enarvades/fil_backend/notebooks/ensembling/model_repository/model_RFC-cpu/config.pbtxt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_config(model_logistic_dir, deployment_type='gpu', model_format='xgboost_json', storage_type='AUTO')\n",
    "generate_config(model_logistic_cpu_dir, deployment_type='cpu', model_format='xgboost_json', storage_type='AUTO')\n",
    "generate_config(model_oversample_dir, deployment_type='gpu', model_format='xgboost_json', storage_type='SPARSE')\n",
    "generate_config(model_oversample_cpu_dir, deployment_type='cpu', model_format='xgboost_json', storage_type='SPARSE')\n",
    "generate_config(model_RFC_dir, deployment_type='gpu', model_format='treelite_checkpoint', storage_type='AUTO')\n",
    "generate_config(model_RFC_cpu_dir, deployment_type='cpu', model_format='treelite_checkpoint', storage_type='AUTO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd8a43",
   "metadata": {},
   "source": [
    "## Ensembling\n",
    "Triton's [Python backend](https://github.com/triton-inference-server/python_backend) is the what we will be utilizing in order to implement the actual ensembling. Therefore, the ensembling will occur on Triton instead of locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e77ba2",
   "metadata": {},
   "source": [
    "### The Python Model\n",
    "Here is where we will implement the ensembling for our models. We are able to execute inference requests on our other models while executing the Python model. We will be building off of this [example](https://github.com/triton-inference-server/python_backend/tree/main/examples/bls) in order to implement our Python model. \n",
    "\n",
    "For the ensembling process, we must first execute requests on each of our models. For each model, we send an asynchronous request to Triton. Each request is saved in `inference_response_awaits`, and then `inference_responses` when each request is complete.\n",
    "\n",
    "Finally, we convert our output tensors to numpy and perform ensembling on the numpy arrays, just like we would do locally. The final ensembled tensor is then returned as a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0df2d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_text = \"\"\"import triton_python_backend_utils as pb_utils\n",
    "import json+\n",
    "import asyncio\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "\n",
    "class TritonPythonModel:\n",
    "    def initialize(self, args): \n",
    "        self.model_config = json.loads(args['model_config'])\n",
    "\n",
    "    async def execute(self, requests):\n",
    "\n",
    "        responses = []\n",
    "\n",
    "        for request in requests:\n",
    "            in_0 = pb_utils.get_input_tensor_by_name(request, \"input__0\")\n",
    "            deployment_type = pb_utils.get_input_tensor_by_name(request, \"deployment_type\")\n",
    "\n",
    "            GPU_models = ['model_logistic', 'model_oversample', 'model_RFC']\n",
    "            CPU_models = ['model_logistic-cpu', 'model_oversample-cpu', 'model_RFC-cpu']\n",
    "\n",
    "            models = GPU_models if deployment_type == \"GPU\" else CPU_models\n",
    "\n",
    "            inference_response_awaits = []\n",
    "\n",
    "            for model_name in models:\n",
    "                infer_request = pb_utils.InferenceRequest(\n",
    "                    model_name=model_name,\n",
    "                    requested_output_names=[\"output__0\"],\n",
    "                    inputs=[in_0])\n",
    "\n",
    "                inference_response_awaits.append(infer_request.async_exec())\n",
    "\n",
    "            inference_responses = await asyncio.gather(\n",
    "                *inference_response_awaits)\n",
    "\n",
    "            for infer_response in inference_responses:\n",
    "                if infer_response.has_error():\n",
    "                    raise pb_utils.TritonModelException(\n",
    "                        infer_response.error().message())\n",
    "\n",
    "            logistic_tensor = pb_utils.get_output_tensor_by_name(\n",
    "                inference_responses[0], \"output__0\")\n",
    "\n",
    "            oversample_tensor = pb_utils.get_output_tensor_by_name(\n",
    "                inference_responses[1], \"output__0\")\n",
    "            \n",
    "            RFC_tensor = pb_utils.get_output_tensor_by_name(\n",
    "                inference_responses[2], \"output__0\")\n",
    "\n",
    "            logistic_tensor = logistic_tensor.to_dlpack()\n",
    "            oversample_tensor = oversample_tensor.to_dlpack()\n",
    "            RFC_tensor = RFC_tensor.to_dlpack()\n",
    "\n",
    "            class DLPack:\n",
    "                def __init__(self, tensor):\n",
    "                    self.tensor = tensor\n",
    "                \n",
    "                def __dlpack__(self):\n",
    "                    return self.tensor\n",
    "                \n",
    "                def __dlpack_device__(self):\n",
    "                    return self.tensor.memory_type_id\n",
    "\n",
    "            logistic_tensor = DLPack(logistic_tensor)\n",
    "            oversample_tensor = DLPack(oversample_tensor)\n",
    "            RFC_tensor = DLPack(RFC_tensor)\n",
    "\n",
    "            def DLPack_to_numpy(tensor):\n",
    "                if tensor.is_cpu():\n",
    "                    return tensor.as_numpy()\n",
    "                else:\n",
    "                    pytorch_tensor = from_dlpack(tensor)\n",
    "                return pytorch_tensor.cpu().numpy()\n",
    "\n",
    "            ensemble = (DLPack_to_numpy(logistic_tensor) + DLPack_to_numpy(oversample_tensor) + DLPack_to_numpy(RFC_tensor)) / 3\n",
    "            ensembled_tensor = pb_utils.Tensor(\"output__0\", ensemble)\n",
    "\n",
    "            inference_response = pb_utils.InferenceResponse(\n",
    "                output_tensors=[ensembled_tensor])\n",
    "            responses.append(inference_response)\n",
    "\n",
    "        return responses\n",
    "\n",
    "    def finalize(self):\n",
    "        print('Cleaning up...')\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "# We will create a new directory for this model called \"model_ensemble\".\n",
    "python_dir = os.path.join(REPO_PATH, 'model_ensemble')\n",
    "python_version_dir = os.path.join(python_dir, '1')\n",
    "os.makedirs(python_version_dir, exist_ok=True)\n",
    "\n",
    "# Next, we write out the python model to the directory we created. \n",
    "python_path = os.path.join(python_version_dir, 'model.py')\n",
    "with open(python_path, 'w') as file_:\n",
    "    file_.write(python_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a50d3b1",
   "metadata": {},
   "source": [
    "### The Configuration File\n",
    "The Python backend requires a configuration file in order to deploy on Triton. To use this backend, we set the `backend` field of this file to `\"python\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "179b5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = (5, 393)\n",
    "num_classes = cp.unique(y_test).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "035f9478",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_text = f\"\"\"name: \"model_ensemble\"\n",
    "backend: \"python\"\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {features} ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"deployment_type\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "output [\n",
    "  {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {num_classes} ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "parameters: {{\n",
    "  key: \"EXECUTION_ENV_PATH\",\n",
    "  value: {{string_value: \"$$TRITON_MODEL_DIRECTORY/pytorch.tar.gz\"}}\n",
    "}}\n",
    "\n",
    "instance_group [{{ kind: KIND_CPU }}]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "config_path = os.path.join(python_dir, 'config.pbtxt')\n",
    "with open(config_path, 'w') as file_:\n",
    "    file_.write(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016e3d5",
   "metadata": {},
   "source": [
    "### Starting the server\n",
    "Like in the previous notebook, we will start the server, wait until it comes online, and check the logs for warnings or errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f9e2f6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/tritonserver\" is already in use by container \"8fcce29e2f3b3fdc0d1437b902b1ff5547836bfe65c4678c37222346c3254ba9\". You have to remove (or rename) that container to be able to reuse that name.\r\n",
      "See 'docker run --help'.\r\n"
     ]
    }
   ],
   "source": [
    "!docker run --gpus all -d -p 8000:8000 -p 8001:8001 -p 8002:8002 -v {REPO_PATH}:/models --name tritonserver {TRITON_IMAGE} tritonserver --model-repository=/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd2c07ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tritonclient.grpc as triton_grpc\n",
    "from tritonclient import utils as triton_utils\n",
    "HOST = 'localhost'\n",
    "PORT = 8001\n",
    "TIMEOUT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0c7972b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "client = triton_grpc.InferenceServerClient(url=f'{HOST}:{PORT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66742762",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(30) # Wait for server to come online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca7e9196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "=============================\r\n",
      "== Triton Inference Server ==\r\n",
      "=============================\r\n",
      "\r\n",
      "NVIDIA Release 22.05 (build 38317651)\r\n",
      "Triton Server Version 2.22.0\r\n",
      "\r\n",
      "Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\r\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n",
      "\r\n",
      "WARNING: CUDA Minor Version Compatibility mode ENABLED.\r\n",
      "  Using driver version 495.29.05 which has support for CUDA 11.5.  This container\r\n",
      "  was built with CUDA 11.7 and will be run in Minor Version Compatibility mode.\r\n",
      "  CUDA Forward Compatibility is preferred over Minor Version Compatibility for use\r\n",
      "  with this container but was unavailable:\r\n",
      "  [[System has unsupported display driver / cuda driver combination (CUDA_ERROR_SYSTEM_DRIVER_MISMATCH) cuInit()=803]]\r\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\r\n",
      "\r\n",
      "I0819 19:56:10.007630 1 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f0e42000000' with size 268435456\r\n",
      "I0819 19:56:10.010269 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\n",
      "I0819 19:56:10.010276 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 1 with size 67108864\r\n",
      "I0819 19:56:10.010280 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 2 with size 67108864\r\n",
      "I0819 19:56:10.010283 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 3 with size 67108864\r\n",
      "[libprotobuf ERROR /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo/src/grpc/third_party/protobuf/src/google/protobuf/text_format.cc:317] Error parsing text-format inference.ModelConfig: 8:13: Expected integer, got: (\r\n",
      "E0819 19:56:10.325689 1 model_repository_manager.cc:2063] Poll failed for model directory 'model_ensemble': failed to read text proto from /models/model_ensemble/config.pbtxt\r\n",
      "I0819 19:56:10.350979 1 model_repository_manager.cc:1191] loading: model_oversample:1\r\n",
      "I0819 19:56:10.451817 1 model_repository_manager.cc:1191] loading: model_RFC-cpu:1\r\n",
      "I0819 19:56:10.472196 1 initialize.hpp:43] TRITONBACKEND_Initialize: fil\r\n",
      "I0819 19:56:10.472212 1 backend.hpp:47] Triton TRITONBACKEND API version: 1.9\r\n",
      "I0819 19:56:10.472216 1 backend.hpp:52] 'fil' TRITONBACKEND API version: 1.9\r\n",
      "I0819 19:56:10.473992 1 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: model_oversample (version 1)\r\n",
      "I0819 19:56:10.476413 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_oversample_0 (GPU device 0)\r\n",
      "I0819 19:56:10.552696 1 model_repository_manager.cc:1191] loading: model_RFC:1\r\n",
      "I0819 19:56:10.653318 1 model_repository_manager.cc:1191] loading: model_logistic-cpu:1\r\n",
      "I0819 19:56:10.714279 1 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: model_RFC-cpu (version 1)\r\n",
      "I0819 19:56:10.715161 1 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: model_RFC (version 1)\r\n",
      "I0819 19:56:10.716603 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_oversample_0 (GPU device 1)\r\n",
      "I0819 19:56:10.754146 1 model_repository_manager.cc:1191] loading: model_logistic:1\r\n",
      "I0819 19:56:10.854929 1 model_repository_manager.cc:1191] loading: model_oversample-cpu:1\r\n",
      "I0819 19:56:10.880023 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_RFC-cpu_0 (CPU device 0)\r\n",
      "I0819 19:56:10.888773 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_RFC_0 (GPU device 0)\r\n",
      "I0819 19:56:10.888918 1 model_repository_manager.cc:1345] successfully loaded 'model_RFC-cpu' version 1\r\n",
      "I0819 19:56:11.008351 1 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: model_logistic-cpu (version 1)\r\n",
      "[W] [19:56:10.899508] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\r\n",
      "I0819 19:56:11.010094 1 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: model_logistic (version 1)\r\n",
      "I0819 19:56:11.012136 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_logistic-cpu_0 (CPU device 0)\r\n",
      "I0819 19:56:11.191101 1 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: model_oversample-cpu (version 1)\r\n",
      "I0819 19:56:11.191369 1 model_repository_manager.cc:1345] successfully loaded 'model_logistic-cpu' version 1\r\n",
      "I0819 19:56:11.192086 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_RFC_0 (GPU device 1)\r\n",
      "[W] [19:56:11.206596] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\r\n",
      "I0819 19:56:11.317382 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_oversample_0 (GPU device 2)\r\n",
      "I0819 19:56:11.482346 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_logistic_0 (GPU device 0)\r\n",
      "I0819 19:56:11.686713 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_oversample-cpu_0 (CPU device 0)\r\n",
      "I0819 19:56:11.833339 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_RFC_0 (GPU device 2)\r\n",
      "I0819 19:56:11.833500 1 model_repository_manager.cc:1345] successfully loaded 'model_oversample-cpu' version 1\r\n",
      "[W] [19:56:11.846123] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\r\n",
      "I0819 19:56:11.954008 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_oversample_0 (GPU device 3)\r\n",
      "I0819 19:56:12.119383 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_logistic_0 (GPU device 1)\r\n",
      "I0819 19:56:12.119607 1 model_repository_manager.cc:1345] successfully loaded 'model_oversample' version 1\r\n",
      "I0819 19:56:12.311264 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_RFC_0 (GPU device 3)\r\n",
      "I0819 19:56:12.432771 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_logistic_0 (GPU device 2)\r\n",
      "I0819 19:56:12.432985 1 model_repository_manager.cc:1345] successfully loaded 'model_RFC' version 1\r\n",
      "[W] [19:56:12.324219] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\r\n",
      "I0819 19:56:12.660155 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_logistic_0 (GPU device 3)\r\n",
      "I0819 19:56:12.859471 1 model_repository_manager.cc:1345] successfully loaded 'model_logistic' version 1\r\n",
      "I0819 19:56:12.859597 1 server.cc:556] \r\n",
      "+------------------+------+\r\n",
      "| Repository Agent | Path |\r\n",
      "+------------------+------+\r\n",
      "+------------------+------+\r\n",
      "\r\n",
      "I0819 19:56:12.859686 1 server.cc:583] \r\n",
      "+---------+-------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Backend | Path                                            | Config                                                                                                                                                         |\r\n",
      "+---------+-------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| fil     | /opt/tritonserver/backends/fil/libtriton_fil.so | {\"cmdline\":{\"auto-complete-config\":\"false\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\r\n",
      "+---------+-------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0819 19:56:12.859757 1 server.cc:626] \r\n",
      "+----------------------+---------+--------+\r\n",
      "| Model                | Version | Status |\r\n",
      "+----------------------+---------+--------+\r\n",
      "| model_RFC            | 1       | READY  |\r\n",
      "| model_RFC-cpu        | 1       | READY  |\r\n",
      "| model_logistic       | 1       | READY  |\r\n",
      "| model_logistic-cpu   | 1       | READY  |\r\n",
      "| model_oversample     | 1       | READY  |\r\n",
      "| model_oversample-cpu | 1       | READY  |\r\n",
      "+----------------------+---------+--------+\r\n",
      "\r\n",
      "I0819 19:56:12.935932 1 metrics.cc:650] Collecting metrics for GPU 0: Tesla T4\r\n",
      "I0819 19:56:12.935962 1 metrics.cc:650] Collecting metrics for GPU 1: Tesla T4\r\n",
      "I0819 19:56:12.935969 1 metrics.cc:650] Collecting metrics for GPU 2: Tesla T4\r\n",
      "I0819 19:56:12.935975 1 metrics.cc:650] Collecting metrics for GPU 3: Tesla T4\r\n",
      "I0819 19:56:12.936795 1 tritonserver.cc:2138] \r\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Option                           | Value                                                                                                                                                                                        |\r\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| server_id                        | triton                                                                                                                                                                                       |\r\n",
      "| server_version                   | 2.22.0                                                                                                                                                                                       |\r\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |\r\n",
      "| model_repository_path[0]         | /models                                                                                                                                                                                      |\r\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                    |\r\n",
      "| strict_model_config              | 1                                                                                                                                                                                            |\r\n",
      "| rate_limit                       | OFF                                                                                                                                                                                          |\r\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |\r\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                     |\r\n",
      "| cuda_memory_pool_byte_size{1}    | 67108864                                                                                                                                                                                     |\r\n",
      "| cuda_memory_pool_byte_size{2}    | 67108864                                                                                                                                                                                     |\r\n",
      "| cuda_memory_pool_byte_size{3}    | 67108864                                                                                                                                                                                     |\r\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                            |\r\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                          |\r\n",
      "| strict_readiness                 | 1                                                                                                                                                                                            |\r\n",
      "| exit_timeout                     | 30                                                                                                                                                                                           |\r\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0819 19:56:12.936809 1 server.cc:257] Waiting for in-flight requests to complete.\r\n",
      "I0819 19:56:12.936822 1 server.cc:273] Timeout 30: Found 0 model versions that have in-flight inferences\r\n",
      "I0819 19:56:12.936831 1 model_repository_manager.cc:1223] unloading: model_oversample-cpu:1\r\n",
      "I0819 19:56:12.936897 1 model_repository_manager.cc:1223] unloading: model_oversample:1\r\n",
      "I0819 19:56:12.936940 1 model_repository_manager.cc:1223] unloading: model_logistic:1\r\n",
      "I0819 19:56:12.936986 1 model_repository_manager.cc:1223] unloading: model_logistic-cpu:1\r\n",
      "I0819 19:56:12.937030 1 model_repository_manager.cc:1223] unloading: model_RFC-cpu:1\r\n",
      "I0819 19:56:12.937094 1 model_repository_manager.cc:1223] unloading: model_RFC:1\r\n",
      "I0819 19:56:12.937128 1 server.cc:288] All models are stopped, unloading models\r\n",
      "I0819 19:56:12.937125 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.937127 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.937139 1 server.cc:295] Timeout 30: Found 6 live models and 0 in-flight non-inference requests\r\n",
      "I0819 19:56:12.937187 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.937216 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.937263 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.937318 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.937482 1 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\r\n",
      "I0819 19:56:12.937541 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.937667 1 model_repository_manager.cc:1328] successfully unloaded 'model_RFC-cpu' version 1\r\n",
      "I0819 19:56:12.937733 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.937780 1 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\r\n",
      "I0819 19:56:12.937944 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.938021 1 model_repository_manager.cc:1328] successfully unloaded 'model_oversample-cpu' version 1\r\n",
      "I0819 19:56:12.938063 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.938103 1 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\r\n",
      "I0819 19:56:12.938237 1 model_repository_manager.cc:1328] successfully unloaded 'model_RFC' version 1\r\n",
      "I0819 19:56:12.938739 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.938772 1 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\r\n",
      "I0819 19:56:12.938996 1 model_repository_manager.cc:1328] successfully unloaded 'model_logistic-cpu' version 1\r\n",
      "I0819 19:56:12.939047 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.943419 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.943510 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.944104 1 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\r\n",
      "I0819 19:56:12.944266 1 model_repository_manager.cc:1328] successfully unloaded 'model_oversample' version 1\r\n",
      "I0819 19:56:12.945049 1 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n",
      "I0819 19:56:12.946520 1 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\r\n",
      "I0819 19:56:12.948960 1 model_repository_manager.cc:1328] successfully unloaded 'model_logistic' version 1\r\n",
      "I0819 19:56:13.937340 1 server.cc:295] Timeout 29: Found 0 live models and 0 in-flight non-inference requests\r\n",
      "error: creating server: Internal - failed to load all models\r\n"
     ]
    }
   ],
   "source": [
    "!docker logs tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ae4ef",
   "metadata": {},
   "source": [
    "## Submitting inference requests\n",
    "The process for submitting inference requests will be slightly different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64db65d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Taking care of categorical features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def convert_to_numpy(df):\n",
    "    df = df.copy()\n",
    "    cat_cols = df.select_dtypes('category').columns\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].cat.codes\n",
    "    for col in df.columns:\n",
    "        df[col] =  pd.to_numeric(df[col], downcast='float')\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60633965",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np_data = convert_to_numpy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_predict(model_name, arr, deployment_type='GPU'):\n",
    "    triton_inputs = [triton_grpc.InferInput('input__0', arr.shape, 'FP32'), \n",
    "                     triton_grpc.InferInput(deployment_type, [1], 'STRING')]\n",
    "    triton_inputs[0].set_data_from_numpy(arr)\n",
    "    triton_output = triton_grpc.InferRequestedOutput('output__0')\n",
    "    response = client.infer(model_name, inputs=triton_inputs, outputs=[triton_output])\n",
    "    return response.as_numpy('output__0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d5046",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "triton_result = triton_predict('model_ensemble', np_data[0:5])\n",
    "local_result = (model_logistic.predict_proba(X_test[0:5]) + \n",
    "                model_oversample.predict_proba(X_test[0:5]) + \n",
    "                model_RFC.predict_proba(X_test[0:5])) / 3\n",
    "\n",
    "print(\"Result computed on Triton: \")\n",
    "print(triton_result)\n",
    "\n",
    "print(\"Resulted computed locally: \")\n",
    "print(local_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2c8cf37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tritonserver\r\n"
     ]
    }
   ],
   "source": [
    "# Shut down the server\n",
    "!docker rm -f tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698bc9c",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653111f",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "We have demonstrated how to use Triton's FIL backend and Python backend in order to handle an ensemble of models. While we focus on averaging in the example, any ensembling technique could be implemented with the steps above. Furthermore, any type of model can be utilized, including XGBoost, cuML, Scikit-Learn, LightGBM, and any format that can be converted to Treelite's checkpoint format. If your python model requires additional dependencies, the python backend supports [custom python execution environments](https://github.com/triton-inference-server/python_backend#using-custom-python-execution-environments).\n",
    "\n",
    "For more information, we recommend viewing the [FIL backend documentation](https://github.com/triton-inference-server/fil_backend#triton-inference-server-fil-backend) as well as the [Python backend documentation](https://github.com/triton-inference-server/python_backend)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
