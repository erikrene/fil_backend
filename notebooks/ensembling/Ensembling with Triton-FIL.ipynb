{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ad97cc",
   "metadata": {},
   "source": [
    "# Ensembling with Triton-FIL\n",
    "\n",
    "## Introduction\n",
    "This notebook will go through step-by-step the process of training an ensemble of models and deploying it to Triton's new FIL backend, building off of the [Fraud Detection Notebook](https://github.com/triton-inference-server/fil_backend/blob/main/notebooks/categorical-fraud-detection/Fraud_Detection_Example.ipynb). We will be utilizing Triton's Python Backend in order to combine the models. This requires the use of a Python model that executes inference requests on each model in the ensemble. This notebook will focus on the process of creating the Python model, as well as how to submit requests with the Triton Python client.\n",
    "\n",
    "__NOTE__: Currently, GPU support is not available for the Python backend. This is due to a DLPack update that has yet to be implemented. However, the process would be virtually the same, except the Python backend Tensor objects would have to be converted to DLPack with the methods found [here](https://github.com/triton-inference-server/fil_backend/blob/main/notebooks/categorical-fraud-detection/Fraud_Detection_Example.ipynb).\n",
    "\n",
    "## Pre-Requisites\n",
    "This notebook assumes that you have Docker plus a few Python dependencies, as in the Fraud Detection Notebook. However, this notebook utilizes Treelite, as we will be using Scikit-Learn models (these are not natively supported by the FIL backend, unlike XGBoost models). To install all of these dependencies in a conda environment, you may make use of the following conda environment file:\n",
    "```yaml\n",
    "---\n",
    "name: triton_ensemble_nb\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - nvidia\n",
    "  - rapidsai\n",
    "dependencies:\n",
    "  - cudatoolkit=11.4\n",
    "  - cudf=21.12\n",
    "  - cuml=21.12\n",
    "  - cupy\n",
    "  - jupyter\n",
    "  - kaggle\n",
    "  - matplotlib\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - pip\n",
    "  - python=3.8\n",
    "  - scikit-learn\n",
    "  - pip:\n",
    "      - treelite=2.3.0\n",
    "      - tritonclient[all]\n",
    "      - xgboost>=1.5,<1.6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29dd53a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TRITON_IMAGE = 'nvcr.io/nvidia/tritonserver:22.05-py3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190453b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.05-py3: Pulling from nvidia/tritonserver\n",
      "Digest: sha256:a85daa2907f46e70b3782818a0331df62d9b4e0b1f15f1530b2a52c8c782d46d\n",
      "Status: Image is up to date for nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "nvcr.io/nvidia/tritonserver:22.05-py3\n"
     ]
    }
   ],
   "source": [
    "!docker pull {TRITON_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ae4a2",
   "metadata": {},
   "source": [
    "## Fetching Training Data\n",
    "As in the Fraud Detection Notebook, we will make use of data from the [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection/overview) Kaggle competition.\n",
    "\n",
    "**NOTE**: You will need to make sure that your Kaggle credentials are [available](https://github.com/Kaggle/kaggle-api#api-credentials) either through a kaggle.json file or via environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd826c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ieee-fraud-detection.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Archive:  ieee-fraud-detection.zip\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c ieee-fraud-detection\n",
    "!unzip -u ieee-fraud-detection.zip\n",
    "train_csv = 'train_transaction.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0fb66a",
   "metadata": {},
   "source": [
    "## Training Example Models\n",
    "We will be training two XGBoost models and one Scikit-Learn model for our ensemble. The first XGBoost model function is the same as the one from the fraud detection notebook. The second XGBoost function is similar to the first, except it implements random oversampling on the data. The third model is a Scikit-Learn Random Forest Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4557f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.preprocessing import SimpleImputer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b440e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading data\n",
    "data = cudf.read_csv(train_csv)\n",
    "\n",
    "# Replace NaNs in data\n",
    "nan_columns = data.columns[data.isna().any().to_pandas()]\n",
    "float_nan_subset = data[nan_columns].select_dtypes(include='float64')\n",
    "\n",
    "imputer = SimpleImputer(missing_values=cp.nan, strategy='mean')\n",
    "data[float_nan_subset.columns] = imputer.fit_transform(float_nan_subset)\n",
    "\n",
    "obj_nan_subset = data[nan_columns].select_dtypes(include='object')\n",
    "data[obj_nan_subset.columns] = obj_nan_subset.fillna('UNKNOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce2d23e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform label encoding\n",
    "cat_columns = data.select_dtypes(include='object')\n",
    "for col in cat_columns.columns:\n",
    "    data[col] = LabelEncoder().fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f51e89d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = data.drop('isFraud', axis=1)\n",
    "y = data.isFraud.astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.to_pandas(), y.to_pandas(), test_size=0.3, stratify=y.to_pandas(), random_state=SEED\n",
    ")\n",
    "# Copy data to avoid slowdowns due to fragmentation\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4613acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aec11f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model_logistic(num_trees, max_depth):\n",
    "    model = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',\n",
    "        enable_categorical=False,\n",
    "        use_label_encoder=False,\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        objective='binary:logistic',\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=num_trees\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b60edad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model_oversample(num_trees, max_depth):\n",
    "    model = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',\n",
    "        enable_categorical=False,\n",
    "        use_label_encoder=False,\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        objective='binary:logistic',\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=num_trees\n",
    "    )\n",
    "    \n",
    "    oversample = RandomOverSampler(sampling_strategy=0.5) # Define oversampling strategy\n",
    "    X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "    \n",
    "    model.fit(\n",
    "        X_over,\n",
    "        y_over,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95b6328b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model_RFC(num_trees, max_depth):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=num_trees,\n",
    "        max_depth=max_depth,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29691d00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_logistic = train_model_logistic(1500, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10275e39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_oversample = train_model_oversample(500, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6956b8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_RFC = train_model_RFC(40, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65e8ab0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeing up room on GPU\n",
    "import gc\n",
    "del data\n",
    "del nan_columns\n",
    "del float_nan_subset\n",
    "del imputer\n",
    "del obj_nan_subset\n",
    "del cat_columns\n",
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3008720",
   "metadata": {},
   "source": [
    "## Preparing Models for Deployment\n",
    "The same process in the previous notebook will be used to prepare models to be deployed to Triton. First, we will serialize the models, then add the configuration files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e0a5c",
   "metadata": {},
   "source": [
    "### Model Serialization\n",
    "Once again, certain model formats are not natively supported by the FIL backend, so they must be directly serialized to Treelite's checkpoint format. Additionally, it is important to ensure that the correct filename is given to each model, depending on its format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a005e16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import treelite\n",
    "import pickle\n",
    "\n",
    "# Create the model repository directory. The name of this directory is arbitrary.\n",
    "REPO_PATH = os.path.abspath('model_repository')\n",
    "os.makedirs(REPO_PATH, exist_ok=True)\n",
    "\n",
    "# We will use the following variables to record information from the serialization\n",
    "# process that we will require later\n",
    "model_path = None\n",
    "model_format = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ee0f16d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def serialize_model_xgb(model, model_name):\n",
    "    model_dir = os.path.join(REPO_PATH, model_name) # Creating model repository\n",
    "    version_dir = os.path.join(model_dir, '1') # Creating version 1 directory\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    # This is the default filename for XGBoost models saved in json format. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_file = os.path.join(version_dir, 'xgboost.json')\n",
    "    model.save_model(model_file)\n",
    "    \n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db3e9d45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def serialize_model_skl(model, model_name):\n",
    "    model_dir = os.path.join(REPO_PATH, model_name)\n",
    "    version_dir = os.path.join(model_dir, '1')\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    # Since Treelite provides no compatibility guarantees between different versions, it is recommended that you\n",
    "    # save models in Pickle or Joblib.\n",
    "    archival_path = os.path.join(version_dir, 'model.pkl')\n",
    "    with open(archival_path,\"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    # This is the default filename expected for Treelite checkpoint models. It is recommended\n",
    "    # that you stick with the default to avoid additional configuration.\n",
    "    model_file = os.path.join(version_dir, 'checkpoint.tl')\n",
    "        \n",
    "    tl_model = treelite.sklearn.import_model(model)\n",
    "    tl_model.serialize(model_file)\n",
    "    \n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf33ec4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/enarvades/miniconda3/envs/triton_ensemble_nb/lib/python3.8/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model_logistic_cpu_dir = serialize_model_xgb(model_logistic, 'model_logistic-cpu')\n",
    "model_oversample_cpu_dir = serialize_model_xgb(model_oversample, 'model_oversample-cpu')\n",
    "model_RFC_cpu_dir = serialize_model_skl(model_RFC, 'model_RFC-cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550c7a9",
   "metadata": {},
   "source": [
    "### The Configuration File\n",
    "We will set up the configuration file in the same manner as the previous notebook, except we will add a parameter for model format, since we're using multiple formats (in this case, XGBoost and Scikit-Learn). \n",
    "\n",
    "Once again, you can read about the FIL backend's configuration options [here](https://github.com/triton-inference-server/fil_backend#configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "539ab7b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = X_test.shape[1]\n",
    "num_classes = cp.unique(y_test).size\n",
    "bytes_per_sample = (features + num_classes) * 4\n",
    "max_batch_size = 60_000_000 // bytes_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e794668",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_config(model_dir, model_format, storage_type, deployment_type='gpu'):\n",
    "    if deployment_type.lower() == 'cpu':\n",
    "        instance_kind = 'KIND_CPU'\n",
    "    else:\n",
    "        instance_kind = 'KIND_GPU'\n",
    "        \n",
    "    config_text = f\"\"\"backend: \"fil\"\n",
    "max_batch_size: {max_batch_size}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {features} ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {num_classes} ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: {instance_kind} }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"{model_format}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"{storage_type}\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\"\"\"\n",
    "    config_path = os.path.join(model_dir, 'config.pbtxt')\n",
    "    with open(config_path, 'w') as file_:\n",
    "        file_.write(config_text)\n",
    "\n",
    "    return config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43016177",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nfs/enarvades/fil_backend/notebooks/ensembling/model_repository/model_RFC-cpu/config.pbtxt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_config(model_logistic_cpu_dir, deployment_type='cpu', model_format='xgboost_json', storage_type='AUTO')\n",
    "generate_config(model_oversample_cpu_dir, deployment_type='cpu', model_format='xgboost_json', storage_type='SPARSE')\n",
    "generate_config(model_RFC_cpu_dir, deployment_type='cpu', model_format='treelite_checkpoint', storage_type='AUTO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd8a43",
   "metadata": {},
   "source": [
    "## Ensembling\n",
    "Triton's [Python backend](https://github.com/triton-inference-server/python_backend) is the what we will be utilizing in order to implement the actual ensembling. Therefore, the ensembling will occur on Triton instead of locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3fbd59",
   "metadata": {},
   "source": [
    "### The Python Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1ca584a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"python_path = os.path.join(python_version_dir, 'model.py')\\nwith open(python_path, 'w') as file_:\\n    file_.write(python_text)\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_text = \"\"\"\n",
    "\n",
    "class TritonPythonModel:\n",
    "    def initialize(self, args):\n",
    "\n",
    "        self.model_config = json.loads(args['model_config'])\n",
    "        \n",
    "        self.output0_config = pb_utils.get_output_config_by_name(\n",
    "            self.model_config,'output__0')\n",
    "        \n",
    "        self.output0_dtype = pb_utils.triton_string_to_numpy(\n",
    "            self.output0_config['data_type'])\n",
    "\n",
    "    async def execute(self, requests):\n",
    "        output0_dtype = self.output0_dtype\n",
    "\n",
    "        responses = []\n",
    "        \n",
    "        for request in requests:\n",
    "            in_0 = pb_utils.get_input_tensor_by_name(request, 'input__0')\n",
    "\n",
    "            inference_response_awaits = []\n",
    "            for model_name in ['model_logistic-cpu', 'model_oversample-cpu', 'model_RFC-cpu']:\n",
    "                infer_request = pb_utils.InferenceRequest(\n",
    "                    model_name=model_name,\n",
    "                    requested_output_names=[\"output__0\"],\n",
    "                    inputs=[in_0])\n",
    "                \n",
    "                inference_response_awaits.append(infer_request.async_exec())\n",
    "\n",
    "            inference_responses = await asyncio.gather(\n",
    "                *inference_response_awaits)\n",
    "\n",
    "            for infer_response in inference_responses:\n",
    "                if infer_response.has_error():\n",
    "                    raise pb_utils.TritonModelException(\n",
    "                        infer_response.error().message())\n",
    "\n",
    "            logistic_tensor = pb_utils.get_output_tensor_by_name(\n",
    "                inference_responses[0], \"output__0\")\n",
    "\n",
    "            oversample_tensor = pb_utils.get_output_tensor_by_name(\n",
    "                inference_responses[1], \"output__0\")\n",
    "            \n",
    "            RFC_tensor = pb_utils.get_output_tensor_by_name(\n",
    "                inference_responses[2], \"output__0\")\n",
    "            \n",
    "            ensembled = (logistic_tensor.as_numpy() + oversample_tensor.as_numpy() + RFC_tensor.as_numpy()) / 3\n",
    "            ensembled_tensor = pb_utils.Tensor(\"output__0\", ensembled.astype(output0_dtype))\n",
    "\n",
    "            inference_response = pb_utils.InferenceResponse(\n",
    "                output_tensors=[ensembled_tensor])\n",
    "            responses.append(inference_response)\n",
    "            \n",
    "        return responses\n",
    "\n",
    "    def finalize(self):\n",
    "        print('Cleaning up...')\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "# Now that we have our python model, we will write it out to our model repository. \n",
    "# We will create a new directory for this model called \"model_ensemble\".\n",
    "python_dir = os.path.join(REPO_PATH, 'model_ensemble')\n",
    "python_version_dir = os.path.join(python_dir, '1')\n",
    "os.makedirs(python_version_dir, exist_ok=True)\n",
    "\n",
    "\"\"\"python_path = os.path.join(python_version_dir, 'model.py')\n",
    "with open(python_path, 'w') as file_:\n",
    "    file_.write(python_text)\"\"\" #uncomment this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d866de",
   "metadata": {},
   "source": [
    "### The Configuration File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016e3d5",
   "metadata": {},
   "source": [
    "### Starting the server\n",
    "Like in the previous notebook, we will start the server, wait until it comes online, and check the logs for warnings or errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f9e2f6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1a465c90740afb04d9e29fa34c2d6e3862aa5e5a3ea8d171e380625852864a65\r\n"
     ]
    }
   ],
   "source": [
    "!docker run --gpus all -d -p 8000:8000 -p 8001:8001 -p 8002:8002 -v {REPO_PATH}:/models --name tritonserver {TRITON_IMAGE} tritonserver --model-repository=/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd2c07ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tritonclient.grpc as triton_grpc\n",
    "from tritonclient import utils as triton_utils\n",
    "HOST = 'localhost'\n",
    "PORT = 8001\n",
    "TIMEOUT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d0c7972b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "client = triton_grpc.InferenceServerClient(url=f'{HOST}:{PORT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66742762",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(30) # Wait for server to come online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca7e9196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "=============================\r\n",
      "== Triton Inference Server ==\r\n",
      "=============================\r\n",
      "\r\n",
      "NVIDIA Release 22.05 (build 38317651)\r\n",
      "Triton Server Version 2.22.0\r\n",
      "\r\n",
      "Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\r\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n",
      "\r\n",
      "WARNING: CUDA Minor Version Compatibility mode ENABLED.\r\n",
      "  Using driver version 495.29.05 which has support for CUDA 11.5.  This container\r\n",
      "  was built with CUDA 11.7 and will be run in Minor Version Compatibility mode.\r\n",
      "  CUDA Forward Compatibility is preferred over Minor Version Compatibility for use\r\n",
      "  with this container but was unavailable:\r\n",
      "  [[System has unsupported display driver / cuda driver combination (CUDA_ERROR_SYSTEM_DRIVER_MISMATCH) cuInit()=803]]\r\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\r\n",
      "\r\n",
      "I0819 13:49:43.947492 1 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f9b54000000' with size 268435456\r\n",
      "I0819 13:49:43.950187 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\n",
      "I0819 13:49:43.950194 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 1 with size 67108864\r\n",
      "I0819 13:49:43.950198 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 2 with size 67108864\r\n",
      "I0819 13:49:43.950201 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 3 with size 67108864\r\n",
      "I0819 13:49:44.298891 1 model_repository_manager.cc:1191] loading: model_logistic-cpu:1\r\n",
      "I0819 13:49:44.399788 1 model_repository_manager.cc:1191] loading: model_RFC-cpu:1\r\n",
      "I0819 13:49:44.420066 1 initialize.hpp:43] TRITONBACKEND_Initialize: fil\r\n",
      "I0819 13:49:44.420083 1 backend.hpp:47] Triton TRITONBACKEND API version: 1.9\r\n",
      "I0819 13:49:44.420087 1 backend.hpp:52] 'fil' TRITONBACKEND API version: 1.9\r\n",
      "I0819 13:49:44.421883 1 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: model_logistic-cpu (version 1)\r\n",
      "I0819 13:49:44.424311 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_logistic-cpu_0 (CPU device 0)\r\n",
      "I0819 13:49:44.500695 1 model_repository_manager.cc:1191] loading: model_RFC:1\r\n",
      "I0819 13:49:44.599355 1 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: model_RFC-cpu (version 1)\r\n",
      "I0819 13:49:44.599610 1 model_repository_manager.cc:1345] successfully loaded 'model_logistic-cpu' version 1\r\n",
      "I0819 13:49:44.601495 1 model_repository_manager.cc:1191] loading: model_oversample-cpu:1\r\n",
      "I0819 13:49:44.601502 1 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: model_RFC (version 1)\r\n",
      "I0819 13:49:44.604299 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_RFC-cpu_0 (CPU device 0)\r\n",
      "I0819 13:49:44.617153 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_RFC_0 (GPU device 0)\r\n",
      "I0819 13:49:44.617217 1 model_repository_manager.cc:1345] successfully loaded 'model_RFC-cpu' version 1\r\n",
      "[W] [13:49:44.630184] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\r\n",
      "I0819 13:49:44.702063 1 model_repository_manager.cc:1191] loading: model_logistic:1\r\n",
      "I0819 13:49:44.802682 1 model_repository_manager.cc:1191] loading: model_ensemble:2\r\n",
      "I0819 13:49:44.899995 1 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: model_oversample-cpu (version 1)\r\n",
      "I0819 13:49:44.901443 1 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: model_logistic (version 1)\r\n",
      "I0819 13:49:44.902567 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_RFC_0 (GPU device 1)\r\n",
      "I0819 13:49:44.903531 1 model_repository_manager.cc:1191] loading: model_oversample:1\r\n",
      "[W] [13:49:44.909806] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\r\n",
      "I0819 13:49:45.049165 1 python.cc:2065] Using Python execution env /models/model_ensemble/pytorch.tar.gz\r\n",
      "I0819 13:49:45.049209 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_RFC_0 (GPU device 2)\r\n",
      "[W] [13:49:45.054602] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\r\n",
      "I0819 13:49:45.189698 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_oversample-cpu_0 (CPU device 0)\r\n",
      "I0819 13:49:45.332368 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_logistic_0 (GPU device 0)\r\n",
      "I0819 13:49:45.332621 1 model_repository_manager.cc:1345] successfully loaded 'model_oversample-cpu' version 1\r\n",
      "I0819 13:49:45.526406 1 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: model_oversample (version 1)\r\n",
      "I0819 13:49:45.528102 1 python.cc:2388] TRITONBACKEND_ModelInstanceInitialize: model_ensemble_0 (CPU device 0)\r\n",
      "I0819 13:50:16.270608 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_RFC_0 (GPU device 3)\r\n",
      "I0819 13:50:16.270948 1 model_repository_manager.cc:1345] successfully loaded 'model_ensemble' version 2\r\n",
      "[W] [13:50:16.286972] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\r\n",
      "I0819 13:50:16.422070 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_logistic_0 (GPU device 1)\r\n",
      "I0819 13:50:16.422223 1 model_repository_manager.cc:1345] successfully loaded 'model_RFC' version 1\r\n",
      "I0819 13:50:16.731719 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_oversample_0 (GPU device 0)\r\n",
      "I0819 13:50:16.888958 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_logistic_0 (GPU device 2)\r\n",
      "I0819 13:50:17.077376 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_oversample_0 (GPU device 1)\r\n",
      "I0819 13:50:17.233446 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_logistic_0 (GPU device 3)\r\n",
      "I0819 13:50:17.421360 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_oversample_0 (GPU device 2)\r\n",
      "I0819 13:50:17.421548 1 model_repository_manager.cc:1345] successfully loaded 'model_logistic' version 1\r\n",
      "I0819 13:50:17.626142 1 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: model_oversample_0 (GPU device 3)\r\n",
      "I0819 13:50:17.788374 1 model_repository_manager.cc:1345] successfully loaded 'model_oversample' version 1\r\n",
      "I0819 13:50:17.788580 1 server.cc:556] \r\n",
      "+------------------+------+\r\n",
      "| Repository Agent | Path |\r\n",
      "+------------------+------+\r\n",
      "+------------------+------+\r\n",
      "\r\n",
      "I0819 13:50:17.788675 1 server.cc:583] \r\n",
      "+---------+-------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Backend | Path                                                  | Config                                                                                                                                                         |\r\n",
      "+---------+-------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| fil     | /opt/tritonserver/backends/fil/libtriton_fil.so       | {\"cmdline\":{\"auto-complete-config\":\"false\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\r\n",
      "| python  | /opt/tritonserver/backends/python/libtriton_python.so | {\"cmdline\":{\"auto-complete-config\":\"false\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\r\n",
      "+---------+-------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0819 13:50:17.788743 1 server.cc:626] \r\n",
      "+----------------------+---------+--------+\r\n",
      "| Model                | Version | Status |\r\n",
      "+----------------------+---------+--------+\r\n",
      "| model_RFC            | 1       | READY  |\r\n",
      "| model_RFC-cpu        | 1       | READY  |\r\n",
      "| model_ensemble       | 2       | READY  |\r\n",
      "| model_logistic       | 1       | READY  |\r\n",
      "| model_logistic-cpu   | 1       | READY  |\r\n",
      "| model_oversample     | 1       | READY  |\r\n",
      "| model_oversample-cpu | 1       | READY  |\r\n",
      "+----------------------+---------+--------+\r\n",
      "\r\n",
      "I0819 13:50:17.867267 1 metrics.cc:650] Collecting metrics for GPU 0: Tesla T4\r\n",
      "I0819 13:50:17.867301 1 metrics.cc:650] Collecting metrics for GPU 1: Tesla T4\r\n",
      "I0819 13:50:17.867308 1 metrics.cc:650] Collecting metrics for GPU 2: Tesla T4\r\n",
      "I0819 13:50:17.867315 1 metrics.cc:650] Collecting metrics for GPU 3: Tesla T4\r\n",
      "I0819 13:50:17.868181 1 tritonserver.cc:2138] \r\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Option                           | Value                                                                                                                                                                                        |\r\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| server_id                        | triton                                                                                                                                                                                       |\r\n",
      "| server_version                   | 2.22.0                                                                                                                                                                                       |\r\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |\r\n",
      "| model_repository_path[0]         | /models                                                                                                                                                                                      |\r\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                    |\r\n",
      "| strict_model_config              | 1                                                                                                                                                                                            |\r\n",
      "| rate_limit                       | OFF                                                                                                                                                                                          |\r\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |\r\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                     |\r\n",
      "| cuda_memory_pool_byte_size{1}    | 67108864                                                                                                                                                                                     |\r\n",
      "| cuda_memory_pool_byte_size{2}    | 67108864                                                                                                                                                                                     |\r\n",
      "| cuda_memory_pool_byte_size{3}    | 67108864                                                                                                                                                                                     |\r\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                            |\r\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                          |\r\n",
      "| strict_readiness                 | 1                                                                                                                                                                                            |\r\n",
      "| exit_timeout                     | 30                                                                                                                                                                                           |\r\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0819 13:50:17.870510 1 grpc_server.cc:4589] Started GRPCInferenceService at 0.0.0.0:8001\r\n",
      "I0819 13:50:17.870869 1 http_server.cc:3303] Started HTTPService at 0.0.0.0:8000\r\n",
      "I0819 13:50:17.913021 1 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\r\n"
     ]
    }
   ],
   "source": [
    "!docker logs tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ae4ef",
   "metadata": {},
   "source": [
    "## Submitting inference requests\n",
    "The process for submitting inference requests will be slightly different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c64db65d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Taking care of categorical features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def convert_to_numpy(df):\n",
    "    df = df.copy()\n",
    "    cat_cols = df.select_dtypes('category').columns\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].cat.codes\n",
    "    for col in df.columns:\n",
    "        df[col] =  pd.to_numeric(df[col], downcast='float')\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60633965",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np_data = convert_to_numpy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a8a97009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_predict(model_name, arr):\n",
    "    triton_input = triton_grpc.InferInput('input__0', arr.shape, 'FP32')\n",
    "    triton_input.set_data_from_numpy(arr)\n",
    "    triton_output = triton_grpc.InferRequestedOutput('output__0')\n",
    "    response = client.infer(model_name, inputs=[triton_input], outputs=[triton_output])\n",
    "    \"result = response.get_response()\"\n",
    "    return response.as_numpy('output__0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a6d5046",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result computed on Triton: \n",
      "[[0.99040395 0.00959606]\n",
      " [0.99682015 0.00317986]\n",
      " [0.99790555 0.00209443]\n",
      " [0.97382116 0.02617885]\n",
      " [0.99607044 0.00392957]]\n",
      "Resulted computed locally: \n",
      "[[0.99040398 0.00959606]\n",
      " [0.99682016 0.00317986]\n",
      " [0.99790559 0.00209443]\n",
      " [0.97382112 0.02617885]\n",
      " [0.99607044 0.00392957]]\n"
     ]
    }
   ],
   "source": [
    "triton_result = triton_predict('model_ensemble', np_data[0:5])\n",
    "local_result = (model_logistic.predict_proba(X_test[0:5]) + model_oversample.predict_proba(X_test[0:5]) + model_RFC.predict_proba(X_test[0:5])) / 3\n",
    "\n",
    "print(\"Result computed on Triton: \")\n",
    "print(triton_result)\n",
    "\n",
    "print(\"Resulted computed locally: \")\n",
    "print(local_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2c8cf37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tritonserver\r\n"
     ]
    }
   ],
   "source": [
    "# Shut down the server\n",
    "!docker rm -f tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698bc9c",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653111f",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "We have demonstrated how to use Triton's FIL backend and Python backend in order to handle an ensemble of models. While we focus on averaging in the example, any ensembling technique could be implemented with the steps above. Furthermore, any type of model can be utilized, including XGBoost, cuML, Scikit-Learn, LightGBM, and any format that can be converted to Treelite's checkpoint format. If your python model requires additional dependencies, the python backend supports [custom python execution environments](https://github.com/triton-inference-server/python_backend#using-custom-python-execution-environments).\n",
    "\n",
    "For more information, we recommend viewing the [FIL backend documentation](https://github.com/triton-inference-server/fil_backend#triton-inference-server-fil-backend) as well as the [Python backend documentation](https://github.com/triton-inference-server/python_backend)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
